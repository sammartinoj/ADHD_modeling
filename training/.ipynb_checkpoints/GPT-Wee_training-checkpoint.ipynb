{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e013210b",
   "metadata": {},
   "source": [
    "# Training GPT-Wee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b20ad",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9437fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer)\n",
    "from tqdm import tqdm \n",
    "from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4e385",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab1208c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "80ca9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = normalizers.Sequence([NFD(), Lowercase(), Strip(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2b7f0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e72d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f9cb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfiles = [\"babylm_data/aochildes_with_few_interruptions.train\",\n",
    "                 \"babylm_data/babylm_100M/bnc_spoken.train\",\n",
    "                 \"babylm_data/babylm_100M/cbt.train\",\n",
    "                 \"babylm_data/babylm_100M/children_stories.train\",\n",
    "                 \"babylm_data/babylm_100M/gutenberg.train\",\n",
    "                 \"babylm_data/babylm_100M/open_subtitles.train\",\n",
    "                 \"babylm_data/babylm_100M/qed.train\",\n",
    "                 \"babylm_data/babylm_100M/simple_wikipedia.train\",\n",
    "                 \"babylm_data/babylm_100M/switchboard.train\",\n",
    "                 \"babylm_data/babylm_100M/wikipedia.train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "860eb884-b98d-4d0e-83ee-cad832444746",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=8000, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7dad1f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files = textfiles, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19f57718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27cd369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "817d568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08d4c599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in gpt-wee-tokenizer-few-interruptions-v2/tokenizer_config.json\n",
      "Special tokens file saved in gpt-wee-tokenizer-few-interruptions-v2/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gpt-wee-tokenizer-few-interruptions-v2/tokenizer_config.json',\n",
       " 'gpt-wee-tokenizer-few-interruptions-v2/special_tokens_map.json',\n",
       " 'gpt-wee-tokenizer-few-interruptions-v2/tokenizer.json')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.save_pretrained(\"gpt-wee-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5ef16",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dff5b",
   "metadata": {},
   "source": [
    "Load tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f218d024-f1a0-4ca1-83b5-16ba97804b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 15:35:36.260685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-30 15:35:36.866006: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-30 15:35:45.865892: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-04-30 15:35:45.866178: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-04-30 15:35:45.866187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt-wee-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc7af9",
   "metadata": {},
   "source": [
    "#### For regular learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57762c85",
   "metadata": {},
   "source": [
    "Load files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46009dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = [\"babylm_data/aochildes_with_few_interruptions.train\",\n",
    "                  \"babylm_data/babylm_100M/cbt.train\",\n",
    "                 \"babylm_data/babylm_100M/children_stories.train\",\n",
    "                 \"babylm_data/babylm_100M/gutenberg.train\",\n",
    "                 \"babylm_data/babylm_100M/bnc_spoken.train\",\n",
    "                 \"babylm_data/babylm_100M/open_subtitles.train\",\n",
    "                 \"babylm_data/babylm_100M/qed.train\",\n",
    "                 \"babylm_data/babylm_100M/simple_wikipedia.train\",\n",
    "                 \"babylm_data/babylm_100M/switchboard.train\",\n",
    "                 \"babylm_data/babylm_100M/wikipedia.train\"]\n",
    "\n",
    "eval_files = [\"babylm_data/babylm_dev/aochildes.dev\",\n",
    "             \"babylm_data/babylm_dev/cbt.dev\",\n",
    "             \"babylm_data/babylm_dev/children_stories.dev\",\n",
    "             \"babylm_data/babylm_dev/gutenberg.dev\",\n",
    "             \"babylm_data/babylm_dev/bnc_spoken.dev\",\n",
    "             \"babylm_data/babylm_dev/open_subtitles.dev\",\n",
    "             \"babylm_data/babylm_dev/qed.dev\",\n",
    "             \"babylm_data/babylm_dev/simple_wikipedia.dev\",\n",
    "             \"babylm_data/babylm_dev/switchboard.dev\",\n",
    "             \"babylm_data/babylm_dev/wikipedia.dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6cf1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-130374ec83266a28\n",
      "Found cached dataset text (/home/sammartj/.cache/huggingface/datasets/text/default-130374ec83266a28/0.0.0/99cc88223027054f94ce0c7fd69d10eb172910fa0615671283a3c8e5e7af2f9c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810353ac86174a36b06d81d4541431a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset('text', data_files={'train': training_files, \n",
    "                                           'validation': eval_files})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b60c79",
   "metadata": {},
   "source": [
    "Load training data in ```streaming```-mode, so that it gets loaded progressively (quick and dirty implementation of curriculum ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = [\"/home/hamelbur/ordered_text.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e165e6",
   "metadata": {},
   "source": [
    "Ordered text from ```sentence_scoring.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d06e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"text\", data_files={\"train\": training_files, \n",
    "                                           \"validation\": eval_files}, streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebd899",
   "metadata": {},
   "source": [
    "### Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba29f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e7a349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    n_embd = 128,\n",
    "    n_layer = 2,\n",
    "    n_head = 2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39eace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sammartj/.cache/huggingface/datasets/text/default-130374ec83266a28/0.0.0/99cc88223027054f94ce0c7fd69d10eb172910fa0615671283a3c8e5e7af2f9c/cache-94755f435224b316.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deea1b24167341339933af9a2e4a5a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/179 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2107266\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 178747\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        #if length == context_length:\n",
    "        input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac85743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d85ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"toy_model_outputs\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    #num_train_epochs=10,\n",
    "    num_train_epochs = 10, # just for testing!\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    #use_mps_device=True, # enable when training on Mac with Apple Silicon\n",
    "    # max_steps = 2000, # enable for curriculum learning, disable for normal\n",
    "    # max_steps = 2000,  # just for testing!\n",
    "    fp16=True,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],#[:8000]['input_ids'],\n",
    "    eval_dataset=tokenized_datasets['validation'],#[:2000]['input_ids'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c964ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sammartj/.local/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 2107266\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 20580\n",
      "  Number of trainable parameters = 1551872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2523' max='20580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2523/20580 23:16 < 2:46:44, 1.80 it/s, Epoch 1.23/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4523d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('gpt-wee-model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
